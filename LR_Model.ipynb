{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "821fe706",
   "metadata": {},
   "source": [
    "## Creating a Linear Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951a0004",
   "metadata": {},
   "source": [
    "### Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d5ae12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "import math\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a182e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "736da892",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Simple linear regression.csv\")\n",
    "x = df['SAT']\n",
    "y = df['GPA']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.3)\n",
    "avg_x = float(statistics.mean(x))\n",
    "avg_y = float(statistics.mean(y))\n",
    "len_x = len(x)\n",
    "len_y = len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7857999a",
   "metadata": {},
   "source": [
    "### The basic math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c3e258",
   "metadata": {},
   "source": [
    "#### Linear Regression is, put it simply, the line graph which best predicts the output given the input based of the pre-existing informations. As any other line graph, this can be presented as:\n",
    "$$\n",
    "Y = a + bX\n",
    "$$\n",
    "#### Where a is a fixed value, b is the slope of the function and X and Y are the input and output, respectely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af00fa9a",
   "metadata": {},
   "source": [
    "#### The key value of a Linear Regression is the slope, which can be manually resolved using this formula:\n",
    "\n",
    "$$\n",
    "b = r \\cdot \\frac{S_y}{S_x}\n",
    "$$\n",
    "\n",
    "\n",
    "#### Where r is the Pearson's Correlation Coeficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3e832f",
   "metadata": {},
   "source": [
    "$$\n",
    "r = \\frac{\n",
    "\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})\n",
    "}{\n",
    "\\sqrt{\n",
    "\\sum_{i=1}^{n} (x_i - \\bar{x})^2\n",
    "\\sum_{i=1}^{n} (y_i - \\bar{y})^2\n",
    "}\n",
    "}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27a7544",
   "metadata": {},
   "source": [
    "#### As a quick overview, you can interpret this formula in sections. \n",
    "#### The top part simply says \"For each point on the table, substract the X value to the average X value, do the same with the Y value and multiply them\". This part of the formula basically places the the point with the average X value as its X value and the average Y value as its Y value at the origin (0, 0). From there, we observe each data point's position: If it has a positive (previously above average) value of X and a positive Y value, the multiplication retuirns a bigger positive number and sums it to the rest. If the multiplication results in a negative number, this value would substract from the sum. After every value goes though this process, we would be left with a number (it can be positive or negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83efa310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1594.2919999999995"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def top_part_Pearson_corr(x, y):\n",
    "    total = 0\n",
    "    for x, y in zip(x, y):\n",
    "        result = (x - avg_x)*(y- avg_y)\n",
    "        total += result\n",
    "    return total\n",
    "    \n",
    "\n",
    "top_part_Pearson_corr(x, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60004c0e",
   "metadata": {},
   "source": [
    "#### The bottom part of the Pearson's correlation coeficient is a bit more complicated\n",
    "$$\n",
    "r = \\frac{\n",
    "\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})\n",
    "}{\n",
    "\\sqrt{\n",
    "\\sum_{i=1}^{n} (x_i - \\bar{x})^2\n",
    "\\sum_{i=1}^{n} (y_i - \\bar{y})^2\n",
    "}\n",
    "}\n",
    "$$\n",
    "#### At the denominator we find the standard deviation of X multplied by the standard deviation of Y.\n",
    "#### Standard deviation is, simply and brutely explain, how much does each value varies from the other and to the mean, meaning high standar deviation is that there is a huge spectrum of values, while a small standard deviation means most values are within a relatively shorter spectrum. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0690d8",
   "metadata": {},
   "source": [
    "#### The reason we use standard deviation in this case is because we don't take into account the magnitude of variables, me mean to measure the pure linear association. To explain it in a more intuitive manner, think that the numerator of Pearson's coeficient dictates how much X and Y move together, while the denominator tells you how much the move in general. So what you end up getting is the real relationship between the variables, discharding their magnitudes. So for example, if the standar deviation is small, a small change in magnitude could mean a large change in relationship. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e24084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2602.7042156453363"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bottom_part_Pearson_corr(x, y):\n",
    "    subtotal_1 = 0\n",
    "    subtotal_2 = 0\n",
    "    for x, y in zip(x, y):\n",
    "        subtotal_1+= math.pow((x-avg_x),2)\n",
    "        subtotal_2+= math.pow((y-avg_y),2)\n",
    "    result = math.sqrt(subtotal_1*subtotal_2)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "bottom_part_Pearson_corr(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4ba2d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_correlation(x, y):\n",
    "    resultado = top_part_Pearson_corr(x, y)/bottom_part_Pearson_corr(x, y)\n",
    "    return resultado\n",
    "\n",
    "r = pearson_correlation(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a7123f",
   "metadata": {},
   "source": [
    "### Now with the Pearson's correlation coefficient we can determine the slope of the linear regression using the previously explain formula:\n",
    "$$\n",
    "b = r \\cdot \\frac{S_y}{S_x}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ad8d264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slope_of_function(r, x, y):\n",
    "    subtotal_x = 0\n",
    "    subtotal_y = 0\n",
    "    for x, y in zip(x, y):\n",
    "        subtotal_x+= math.pow((x-avg_x),2)\n",
    "        subtotal_y+= math.pow((y-avg_y),2)\n",
    "    Sx = math.sqrt(subtotal_x/len_x)\n",
    "    Sy = math.sqrt(subtotal_y/len_y)\n",
    "    slope = r*(Sy/Sx)\n",
    "    return slope\n",
    "\n",
    "b = slope_of_function(r, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb7cbcc",
   "metadata": {},
   "source": [
    "### A key concept of Linear Regression is that this \"line graph\" ALWAYS intercepts a point formed by the average values of X and the average values of Y. With that information, we can finalize the graph with a simple:\n",
    "$$\n",
    "\\bar{y} = a + b\\bar{x}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d35838d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_intercept(slope, avg_x, avg_y):\n",
    "    result = avg_y - (avg_x*slope)\n",
    "    return result\n",
    "\n",
    "a = y_intercept(slope, avg_x, avg_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f423354b",
   "metadata": {},
   "source": [
    "### Now we have all the elements to make predictions, so let's test the Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "21950166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(x_test, slope, y_intercept):\n",
    "    predictions = []\n",
    "    for x in x_test:\n",
    "        predict = y_intercept + (slope * x)\n",
    "        predictions.append(predict)\n",
    "    return predictions\n",
    "\n",
    "predictions = prediction(x_test, b, a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
